================================================================================
TORBIZ DOCKER WORKFLOW DIAGRAM
================================================================================

ARCHITECTURE COMPARISON
-----------------------

WINDOWS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Torbiz App (Windows)         â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Frontend  â”‚â”€â”€â”€â†’â”‚ Rust Backend â”‚ â”‚
â”‚  â”‚   (React)  â”‚    â”‚   (Tauri)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      WSL2      â”‚
                    â”‚    (Linux)     â”‚
                    â”‚                â”‚
                    â”‚  Python 3.11   â”‚
                    â”‚  + Petals      â”‚
                    â”‚  + peft        â”‚
                    â”‚  + accelerate  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


MACOS (NEW):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Torbiz App (macOS)           â”‚
â”‚                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Frontend  â”‚â”€â”€â”€â†’â”‚ Rust Backend â”‚ â”‚
â”‚  â”‚   (React)  â”‚    â”‚   (Tauri)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                           â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚    Docker Container    â”‚
                â”‚       (Linux)          â”‚
                â”‚                        â”‚
                â”‚  Python 3.11           â”‚
                â”‚  + Petals              â”‚
                â”‚  + peft                â”‚
                â”‚  + accelerate          â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


================================================================================
GPU SHARING WORKFLOW (macOS)
================================================================================

FIRST-TIME SETUP:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Step 1: User clicks "Share GPU"
         â”‚
         â†“
Step 2: App checks Docker installed
         â”‚
         â”œâ”€â†’ NOT installed? â†’ Show error + install link
         â”‚
         â†“
Step 3: App checks Docker running
         â”‚
         â”œâ”€â†’ NOT running? â†’ Show error (start Docker Desktop)
         â”‚
         â†“
Step 4: App checks Docker image exists
         â”‚
         â”œâ”€â†’ NOT exists? â†’ Build image (5-10 minutes)
         â”‚
         â†“
Step 5: Install system Python (for direct inference)
         â”‚
         â†“
Step 6: Install Petals on system (for direct inference)
         â”‚
         â†“
Step 7: Setup complete! âœ“


NORMAL GPU SHARING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Action: Click "Share GPU"
         â”‚
         â†“
    [Rust Backend]
         â”‚
         â”œâ”€â†’ Verify Docker running
         â”œâ”€â†’ Verify image exists
         â”œâ”€â†’ Build docker run command
         â”‚
         â†“
    docker run --rm \
      --name torbiz-petals-seeder \
      --network host \
      -v /scripts:/app/scripts:ro \
      -v ~/.cache/huggingface:/root/.cache/huggingface \
      -v ~/.torbiz/logs:/root/.torbiz/logs \
      torbiz-petals-macos:latest \
      python3 /app/scripts/run_petals_seeder.py
         â”‚
         â†“
    [Docker Container]
         â”‚
         â”œâ”€â†’ Start Python script
         â”œâ”€â†’ Load Petals dependencies
         â”œâ”€â†’ Connect to DHT network
         â”œâ”€â†’ Download model shards (cached)
         â”œâ”€â†’ Announce blocks to network
         â”‚
         â†“
    [Serving Model Blocks] ğŸ‰
         â”‚
         â†“ (Logs streamed to UI)
         â”‚
    [User sees progress in app]


STOPPING GPU SHARING:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User Action: Click "Stop Sharing"
         â”‚
         â†“
    [Rust Backend]
         â”‚
         â”œâ”€â†’ docker stop torbiz-petals-seeder
         â”‚
         â†“
    [Docker Container]
         â”‚
         â”œâ”€â†’ Receive SIGTERM
         â”œâ”€â†’ Graceful shutdown
         â”œâ”€â†’ Announce offline to DHT
         â”œâ”€â†’ Clean up resources
         â”‚
         â†“
    [Container removed] âœ“
    (--rm flag auto-removes)


================================================================================
DIRECT INFERENCE WORKFLOW (macOS) - UNCHANGED
================================================================================

User Action: Ask question in chat
         â”‚
         â†“
    [Rust Backend]
         â”‚
         â”œâ”€â†’ Use SYSTEM Python 3
         â”œâ”€â†’ Run run_petals_inference.py
         â”‚
         â†“
    [System Python Process]
         â”‚
         â”œâ”€â†’ Connect to Petals network
         â”œâ”€â†’ Find available model blocks
         â”œâ”€â†’ Stream inference results
         â”‚
         â†“
    [User sees answer in chat] âœ“

Note: Direct inference does NOT use Docker!
      It's faster and simpler with system Python.


================================================================================
DOCKER VOLUME MAPPINGS
================================================================================

HOST (macOS)                          CONTAINER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
/path/to/py/scripts/     â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  /app/scripts/ (read-only)
                                      â”‚
                                      â””â”€â†’ Python scripts always current
                                          No need to rebuild image

~/.cache/huggingface/    â†â”€â”€â”€â”€â”€â”€â”€â”€â†’  /root/.cache/huggingface/
                                      â”‚
                                      â””â”€â†’ Model downloads cached
                                          Not re-downloaded each run

~/.torbiz/logs/          â†â”€â”€â”€â”€â”€â”€â”€â”€â†’  /root/.torbiz/logs/
                                      â”‚
                                      â””â”€â†’ Logs persist after container stops
                                          Accessible on host


================================================================================
DEPENDENCY FLOW
================================================================================

DOCKER IMAGE BUILD:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Dockerfile.macos
      â”‚
      â”œâ”€â†’ FROM python:3.11-slim
      â”œâ”€â†’ RUN pip install torch
      â”œâ”€â†’ RUN pip install git+...petals
      â”œâ”€â†’ RUN pip install peft accelerate
      â”œâ”€â†’ RUN python3 -c "import petals" (verify)
      â”‚
      â†“
 Docker Image: torbiz-petals-macos:latest
      â”‚
      â””â”€â†’ Contains ALL dependencies pre-installed
          No runtime installation needed!


GPU SHARING RUNTIME:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Docker Container Starts
      â”‚
      â”œâ”€â†’ All dependencies already installed âœ“
      â”œâ”€â†’ Python script starts immediately
      â”œâ”€â†’ No pip install delays
      â”‚
      â†“
 Petals seeder runs smoothly


================================================================================
ERROR HANDLING FLOW
================================================================================

                    [User clicks "Share GPU"]
                              â”‚
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Docker installed?â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                       YESâ”‚      â”‚NO
                          â”‚      â””â”€â”€â†’ Error: "Install Docker Desktop"
                          â”‚           Link: https://docker.com/...
                          â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Docker running?  â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                       YESâ”‚      â”‚NO
                          â”‚      â””â”€â”€â†’ Error: "Start Docker Desktop"
                          â”‚           "Check whale icon in menu bar"
                          â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Image exists?    â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                       YESâ”‚      â”‚NO
                          â”‚      â””â”€â”€â†’ Info: "Building image..."
                          â”‚           Build (5-10 min)
                          â”‚           Progress shown in UI
                          â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Start container  â”‚
                    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                  SUCCESSâ”‚      â”‚FAIL
                         â”‚      â””â”€â”€â†’ Error: Show Docker error
                         â”‚           Suggest: Rebuild image
                         â†“
                    [GPU Sharing Active] âœ“


================================================================================
COMPARISON TABLE
================================================================================

Feature              Windows (WSL)        macOS (Docker)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Environment          WSL2 Linux           Docker Container
Python Location      ~/.torbiz_venv       /usr/local/bin/python3
Dependencies         pip install in WSL   Pre-built in image
GPU Sharing          Via WSL              Via Docker
Direct Inference     Via WSL              Via system Python
Setup Time           5-10 minutes         5-10 minutes
Model Cache          WSL filesystem       Mac filesystem
Logs                 WSL filesystem       Mac filesystem
Reliability          âœ“âœ“âœ“                  âœ“âœ“âœ“
Maintenance          Medium               Low (rebuild image)


================================================================================
USER JOURNEY
================================================================================

NEW USER (First Time):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Install Docker Desktop           [5 min]
2. Start Docker Desktop            [1 min]
3. Open Torbiz app                 [instant]
4. Click "Share GPU"               [instant]
5. Wait for setup                  [10-15 min]
   â”œâ”€â†’ Check Docker
   â”œâ”€â†’ Build image (one-time)
   â”œâ”€â†’ Install system Python
   â””â”€â†’ Install Petals
6. Select model                    [instant]
7. Click "Share GPU" again         [instant]
8. Wait for model download         [varies]
9. Start serving blocks            [1-2 min]
   
   âœ“ GPU sharing active!


RETURNING USER:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1. Open Torbiz app                 [instant]
2. Select model                    [instant]
3. Click "Share GPU"               [instant]
4. Container starts                [2-5 sec]
5. Model loads (cached)            [30-60 sec]
6. Start serving blocks            [30 sec]
   
   âœ“ GPU sharing active!
   (Much faster - image + models cached!)


================================================================================
SUCCESS INDICATORS
================================================================================

âœ“ No dependency errors
âœ“ Container starts within 5 seconds
âœ“ Model loads from cache
âœ“ Connects to DHT network
âœ“ Announces blocks successfully
âœ“ Logs stream to UI
âœ“ Stop button works immediately
âœ“ Direct inference unaffected
âœ“ Model cache persists


================================================================================
END OF WORKFLOW DIAGRAM
================================================================================

