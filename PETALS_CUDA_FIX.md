# Petals CUDA Fix - Terminal Window Solution

## üéØ Problem Summary

**Issue**: Petals model hosting worked initially but failed after a few minutes with:
```
CUDA SETUP: CUDA detection failed!
Required library version not found: libbitsandbytes_cuda128.so
```

**Root Cause**: The `LD_LIBRARY_PATH` environment variable was never set in WSL, so when Petals reloaded model blocks after a few minutes, bitsandbytes couldn't find CUDA libraries (`libcudart.so`, etc.).

## ‚úÖ Solution Implemented

### 1. **Set CUDA Library Paths in Python Script** (`src-tauri/py/run_petals_seeder.py`)
   
   **Added before importing Petals:**
   - Automatic detection of CUDA installation paths
   - Setting `LD_LIBRARY_PATH` to include all common CUDA locations:
     - `/usr/local/cuda/lib64`
     - `/usr/local/cuda-12/lib64`
     - `/usr/local/cuda-11/lib64`
     - `/usr/lib/x86_64-linux-gnu`
   - Setting `CUDA_HOME=/usr/local/cuda`
   - Logging of all environment variables for debugging

   **Why this fixes it**: bitsandbytes needs `LD_LIBRARY_PATH` to find CUDA libraries at runtime, especially when Petals reloads blocks.

### 2. **Set Environment Variables in WSL Command** (`src-tauri/src/lib.rs`)
   
   **Modified the WSL launch command to include:**
   ```bash
   export LD_LIBRARY_PATH=/usr/local/cuda/lib64:...:$LD_LIBRARY_PATH
   export CUDA_HOME=/usr/local/cuda
   source ~/.torbiz_venv/bin/activate
   python3 run_petals_seeder.py ...
   ```

   **Why this is important**: Sets the environment BEFORE Python starts, ensuring all child processes inherit these paths.

### 3. **Show Terminal Window** (`src-tauri/src/lib.rs`)
   
   **Removed the `CREATE_NO_WINDOW` flag:**
   - Terminal window is now visible when sharing GPU
   - Users can see real-time logs
   - Better for debugging CUDA issues
   - Prevents WSL subprocess environment corruption

   **User Experience:**
   - Terminal window opens automatically
   - Can be minimized (but not closed)
   - Shows progress and errors in real-time
   - Closes automatically when GPU sharing stops

### 4. **Enhanced CUDA Detection During Setup** (`src-tauri/src/lib.rs`)
   
   **Added checks during WSL setup:**
   - Detects if CUDA toolkit is installed (`nvcc` check)
   - Verifies PyTorch CUDA support
   - Provides warnings if GPU acceleration won't work
   - Logs CUDA version information

### 5. **Updated UI Guidance** (`src/components/ShareGpuModal.jsx`)
   
   **Added informational messages:**
   - Explains that terminal window is normal and improves stability
   - Tells users they can minimize but shouldn't close it
   - Clarifies that the window prevents CUDA library errors

## üîç Why This Works

### The Timeline of the Bug:
1. **Minute 0**: Petals starts, loads model blocks successfully
   - Uses cached/initial CUDA state
   - bitsandbytes initializes once with whatever paths it finds

2. **Minutes 1-5**: Model blocks are announced online
   - Everything works fine
   - Initial CUDA setup is still valid

3. **After a few minutes**: Petals reloads or re-initializes blocks
   - bitsandbytes tries to re-initialize CUDA
   - **WITHOUT `LD_LIBRARY_PATH`**, it can't find `libcudart.so`
   - Falls back to CPU, causing the error

### The Solution:
By setting `LD_LIBRARY_PATH` **before** Python starts and **before** any imports, bitsandbytes can always find CUDA libraries, even when re-initializing.

## üìã Installation/Setup Notes

### First-Time Setup (automatic):
1. User clicks "Share GPU"
2. WSL setup runs automatically
3. **New**: CUDA detection runs, warns if missing
4. Petals installs with PyTorch

### When Starting GPU Sharing:
1. Terminal window opens (this is normal!)
2. Logs show CUDA library paths being set
3. Petals starts and connects to network
4. **No more CUDA errors after a few minutes!**

### Expected Log Output:
```
[INFO] ==================================================
[INFO] CUDA Library Configuration:
[INFO] LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-12/lib64:...
[INFO] CUDA_HOME: /usr/local/cuda
[INFO] ==================================================
[INFO] bitsandbytes configured for CUDA 12.3 compatibility
```

## üö® Troubleshooting

### If CUDA Still Not Working:

1. **Check if CUDA is installed in WSL:**
   ```bash
   wsl nvcc --version
   ```
   If not found, install CUDA in WSL: https://docs.nvidia.com/cuda/wsl-user-guide/

2. **Verify CUDA library files exist:**
   ```bash
   wsl ls -la /usr/local/cuda/lib64/libcudart.so*
   ```

3. **Check PyTorch CUDA support:**
   ```bash
   wsl ~/.torbiz_venv/bin/python3 -c "import torch; print(torch.cuda.is_available())"
   ```

4. **Run bitsandbytes diagnostics:**
   ```bash
   wsl ~/.torbiz_venv/bin/python3 -m bitsandbytes
   ```

### If Terminal Window is Annoying:

- **Just minimize it** - don't close it!
- The window is necessary for stable CUDA environment
- It will close automatically when you stop sharing

## üîß Technical Details

### Files Modified:
1. `src-tauri/py/run_petals_seeder.py` - Added CUDA path detection and LD_LIBRARY_PATH setup
2. `src-tauri/src/lib.rs` - Removed CREATE_NO_WINDOW, added environment exports, enhanced CUDA checks
3. `src/components/ShareGpuModal.jsx` - Added UI guidance about terminal window

### Environment Variables Set:
- `LD_LIBRARY_PATH`: Critical for shared library loading
- `CUDA_HOME`: Helps tools find CUDA installation
- `BNB_CUDA_VERSION`: Already existed, tells bitsandbytes which CUDA version to use

### Why Not Just Install Different bitsandbytes Version?

- You were right - if it was just a version issue, it would fail immediately
- The **delayed failure** proves it's an environment/path issue
- Different versions won't help if the libraries can't be found

## üìä Expected Behavior After Fix

### ‚úÖ Before (Broken):
```
[INFO] Announced that blocks [...] are joining
[SUCCESS] Model loaded
[... 2-5 minutes pass ...]
[ERROR] CUDA SETUP: CUDA detection failed!
[ERROR] Required library not found: libbitsandbytes_cuda128.so
[ERROR] Announced that blocks [...] are offline
```

### ‚úÖ After (Fixed):
```
[INFO] LD_LIBRARY_PATH: /usr/local/cuda/lib64:...
[INFO] CUDA_HOME: /usr/local/cuda
[INFO] Announced that blocks [...] are joining
[SUCCESS] Model loaded
[... 2-5 minutes pass ...]
[... 10 minutes pass ...]
[... 1 hour passes ...]
[INFO] Handling inference requests
[NO ERRORS - keeps running indefinitely]
```

## üéâ Benefits of This Approach

1. **Visible Debugging**: Terminal window shows exactly what's happening
2. **Environment Stability**: Proper CUDA paths prevent library loading issues
3. **Better User Experience**: Users understand what's happening (not hidden)
4. **Future-Proof**: Works with different CUDA versions (11.x, 12.x)
5. **Minimal Changes**: Only touches the critical path, doesn't break existing functionality

## üìù Notes for Future Development

- If you want to hide the terminal again in the future, you **must** ensure LD_LIBRARY_PATH persists in the subprocess environment
- Consider adding a "View Logs" button to show/hide the terminal window programmatically
- Could create a WSL helper script that sets up the environment and then launches Python
- May want to add a system tray notification when model blocks go offline/online

---

**Summary**: The fix addresses the root cause (missing LD_LIBRARY_PATH) rather than treating symptoms (version mismatches). The visible terminal window is a feature, not a bug - it provides transparency and stability.

